{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multimodal RAG with LangChain\n",
    "This notebook demonstrates how to build a multimodal Retrieval-Augmented Generation (RAG) system using LangChain. The system can process both text and images from a PDF document, create summaries, store them in a vector store, and answer questions based on the retrieved context."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup\n",
    "### 1.1. Install Dependencies\n",
    "First, we install all the necessary Python packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -U \"unstructured[all-docs]\" pillow lxml chromadb tiktoken langchain langchain-community langchain-openai langchain-groq python_dotenv langchain-openrouter langchain-huggingface sentence-transformers InstructorEmbedding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2. Imports\n",
    "Import all the required libraries and modules."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, re, time, base64, uuid\n",
    "from dotenv import load_dotenv\n",
    "from IPython.display import Image, display\n",
    "from unstructured.partition.pdf import partition_pdf\n",
    "from langchain_groq import ChatGroq\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.messages import HumanMessage\n",
    "from langchain_core.documents import Document\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.storage import InMemoryStore\n",
    "from langchain.embeddings import HuggingFaceBgeEmbeddings\n",
    "from langchain.retrievers.multi_vector import MultiVectorRetriever\n",
    "from langchain_core.runnables import RunnablePassthrough, RunnableLambda\n",
    "from google.api_core.exceptions import ResourceExhausted\n",
    "from groq import RateLimitError\n",
    "from langchain.chat_models import ChatOpenAI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3. Load Environment Variables\n",
    "Load API keys and other secrets from a `.env` file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()  # take environment variables from .env file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4. Utility Functions\n",
    "Helper function to display base64 encoded images in the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_base64_image(base64_code):\n",
    "    image_data = base64.b64decode(base64_code)\n",
    "    display(Image(data=image_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Data Extraction and Preparation\n",
    "### 2.1. Partition PDF\n",
    "We use `unstructured` to partition the PDF file into chunks of text and extract images and tables. The `hi_res` strategy is used to ensure high-quality extraction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = \"./content/attention.pdf\"     # Path to your PDF file\n",
    "\n",
    "chunks = partition_pdf(\n",
    "    filename=file_path,\n",
    "    infer_table_structure=True,            # Extract tables\n",
    "    strategy=\"hi_res\",                     # Mandatory to infer tables\n",
    "    extract_image_block_types=[\"Image\", \"Table\"],   # Extract images of tables\n",
    "    extract_image_block_to_payload=True,   # Extract base64 for API usage\n",
    "    chunking_strategy=\"by_title\",          # Chunking strategy\n",
    "    max_characters=10000,                  # Max characters per chunk\n",
    "    combine_text_under_n_chars=2000,       # Combine small text chunks\n",
    "    new_after_n_chars=6000,                # Start new chunk after n chars\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2. Separate Elements\n",
    "We separate the extracted elements into three categories: texts, tables, and images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tables = []\n",
    "texts = []\n",
    "images = []\n",
    "\n",
    "for chunk in chunks:\n",
    "    elements = chunk.metadata.orig_elements\n",
    "    for element in elements:\n",
    "        if \"Table\" in str(type(element)):\n",
    "            tables.append(element)\n",
    "        elif \"Image\" in str(type(element)):\n",
    "            images.append(element.metadata.image_base64)\n",
    "\n",
    "    if \"CompositeElement\" in str(type((chunk))):\n",
    "        texts.append(chunk)\n",
    "\n",
    "print(f\"Found {len(texts)} text chunks, {len(tables)} tables, and {len(images)} images.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Content Summarization\n",
    "To handle large documents and diverse content types, we summarize the extracted text, tables, and images. These summaries will be used for retrieval."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1. Text and Table Summarization\n",
    "We use a language model from Groq to create concise summaries of the text chunks and tables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary_prompt_template = \"\"\"\n",
    "You are an assistant tasked with summarizing tables and text.\n",
    "Give a concise summary of the table or text.\n",
    "Respond only with the summary, no additional comment.\n",
    "Do not start your message by saying \"Here is a summary\" or anything like that.\n",
    "Just give the summary as it is.\n",
    "\n",
    "Table or text chunk: {element}\n",
    "\"\"\"\n",
    "summary_prompt = ChatPromptTemplate.from_template(summary_prompt_template)\n",
    "\n",
    "summary_model = ChatGroq(temperature=0.5, model=\"llama-3.1-8b-instant\")\n",
    "\n",
    "summarize_chain = {\"element\": lambda x: x} | summary_prompt | summary_model | StrOutputParser()\n",
    "\n",
    "try:\n",
    "    text_summaries = summarize_chain.batch(texts, {\"max_concurrency\": 1})\n",
    "    tables_html = [table.metadata.text_as_html for table in tables]\n",
    "    table_summaries = summarize_chain.batch(tables_html, {\"max_concurrency\": 1})\n",
    "except RateLimitError as e:\n",
    "    print(f\"Rate limit exceeded: {e}. Waiting for 5 minutes before retrying.\")\n",
    "    time.sleep(300)\n",
    "    text_summaries = summarize_chain.batch(texts, {\"max_concurrency\": 1})\n",
    "    tables_html = [table.metadata.text_as_html for table in tables]\n",
    "    table_summaries = summarize_chain.batch(tables_html, {\"max_concurrency\": 1})\n",
    "\n",
    "print(f\"Summarized {len(text_summaries)} text chunks and {len(table_summaries)} tables.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2. Image Summarization\n",
    "For images, we use a multimodal model (Gemini) to generate detailed descriptions. The prompt includes context that the images are from the \"Attention Is All You Need\" paper."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_prompt_template = \"\"\"Describe the image in detail. For context,\n",
    "                  the image is part of the research paper by google Attention Is All You Need. \n",
    "                  Be specific about graphs, such as bar plots.\"\"\"\n",
    "\n",
    "image_messages = [\n",
    "    (\"user\",[\n",
    "            {\"type\": \"text\", \"text\": image_prompt_template},\n",
    "            {\"type\": \"image_url\",\"image_url\": {\"url\": \"data:image/jpeg;base64,{image}\"}},\n",
    "        ],\n",
    "    )\n",
    "]\n",
    "\n",
    "image_prompt = ChatPromptTemplate.from_messages(image_messages)\n",
    "\n",
    "image_llm = ChatGoogleGenerativeAI(\n",
    "    model=\"gemini-1.5-flash\",\n",
    "    temperature=0.5,\n",
    "    google_api_key=os.getenv(\"GOOGLE_API_KEY\"),\n",
    ")\n",
    "\n",
    "image_chain = image_prompt | image_llm | StrOutputParser()\n",
    "# Desprate attempt to handle free tier limits for llm api calls xD\n",
    "\n",
    "while True:\n",
    "    try:\n",
    "        image_summaries = image_chain.batch(images, {\"max_concurrency\": 1})\n",
    "        break\n",
    "    except ResourceExhausted as e:\n",
    "        msg = str(e)\n",
    "        match = re.search(r'Please try again in ([\\d\\.]+)([sm])', msg)\n",
    "        wait_seconds = 60\n",
    "        if match:\n",
    "            value, unit = match.groups()\n",
    "            wait_seconds = float(value) * 60 if unit == 'm' else float(value)\n",
    "        print(f\"⏳ Rate limit exceeded. Waiting {wait_seconds:.2f} seconds...\")\n",
    "        time.sleep(wait_seconds)\n",
    "    except Exception as e:\n",
    "        print(f\"⚠️ Unexpected error: {e}\")\n",
    "        time.sleep(5)\n",
    "\n",
    "print(f\"Summarized {len(image_summaries)} images.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Vector Store and Retriever Setup\n",
    "### 4.1. Initialize Embeddings and Vector Store\n",
    "We use a multi-vector retriever strategy. The summaries are stored in a Chroma vector store, while the original, larger chunks (parent documents) are stored in an in-memory store."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = HuggingFaceBgeEmbeddings(\n",
    "    model_name=\"BAAI/bge-large-en-v1.5\",\n",
    "    encode_kwargs={\"normalize_embeddings\": True}\n",
    ")\n",
    "\n",
    "vectorstore = Chroma(\n",
    "    collection_name=\"multimodal_rag\",\n",
    "    embedding_function=embeddings\n",
    ")\n",
    "\n",
    "store = InMemoryStore()\n",
    "id_key = \"doc_id\"\n",
    "\n",
    "retriever = MultiVectorRetriever(\n",
    "    vectorstore=vectorstore,\n",
    "    docstore=store,\n",
    "    id_key=id_key,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2. Add Documents to Retriever\n",
    "We add the text, table, and image summaries to the vector store, and the original documents to the document store."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add texts\n",
    "doc_ids = [str(uuid.uuid4()) for _ in texts]\n",
    "summary_texts = [Document(page_content=summary, metadata={id_key: doc_ids[i]}) for i, summary in enumerate(text_summaries)]\n",
    "retriever.vectorstore.add_documents(summary_texts)\n",
    "retriever.docstore.mset(list(zip(doc_ids, texts)))\n",
    "\n",
    "# Add tables\n",
    "table_ids = [str(uuid.uuid4()) for _ in tables]\n",
    "summary_tables = [Document(page_content=summary, metadata={id_key: table_ids[i]}) for i, summary in enumerate(table_summaries)]\n",
    "retriever.vectorstore.add_documents(summary_tables)\n",
    "retriever.docstore.mset(list(zip(table_ids, tables)))\n",
    "\n",
    "# Add image summaries\n",
    "img_ids = [str(uuid.uuid4()) for _ in images]\n",
    "summary_img = [Document(page_content=summary, metadata={id_key: img_ids[i]}) for i, summary in enumerate(image_summaries)]\n",
    "retriever.vectorstore.add_documents(summary_img)\n",
    "retriever.docstore.mset(list(zip(img_ids, images)))\n",
    "\n",
    "print(f\"Total documents in vector store: {vectorstore._collection.count()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Building the RAG Pipeline\n",
    "We build the final RAG chain. The retriever fetches relevant document summaries, and the corresponding full documents (text or images) are passed to the final model to generate an answer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_docs(docs):\n",
    "    b64_images = []\n",
    "    text_docs = []\n",
    "    for doc in docs:\n",
    "        try:\n",
    "            # Attempt to decode to check if it's a base64 string\n",
    "            base64.b64decode(doc, validate=True)\n",
    "            b64_images.append(doc)\n",
    "        except Exception:\n",
    "            text_docs.append(doc)\n",
    "    return {\"images\": b64_images, \"texts\": text_docs}\n",
    "\n",
    "def build_prompt(kwargs):\n",
    "    docs_by_type = kwargs[\"context\"]\n",
    "    user_question = kwargs[\"question\"]\n",
    "\n",
    "    context_text = \".\".join(doc.page_content + \"\\n\" for doc in docs_by_type[\"texts\"])\n",
    "\n",
    "    prompt_template = f\"\"\"\n",
    "    You are an expert assistant. Answer the user's question **only** based on the given context.\n",
    "\n",
    "    Context:\n",
    "    {context_text}\n",
    "\n",
    "    Question:\n",
    "    {user_question}\n",
    "\n",
    "    Provide a clear and concise answer.\n",
    "    \"\"\"\n",
    "\n",
    "    prompt_content = [{\"type\": \"text\", \"text\": prompt_template}]\n",
    "    for image in docs_by_type[\"images\"]:\n",
    "        prompt_content.append({\"type\": \"image_url\", \"image_url\": {\"url\": f\"data:image/jpeg;base64,{image}\"}})\n",
    "    \n",
    "    return [HumanMessage(content=prompt_content)]\n",
    "\n",
    "rag_model = ChatOpenAI(\n",
    "    model=\"qwen/qwen2.5-vl-32b-instruct:free\",\n",
    "    base_url=\"https://openrouter.ai/api/v1\",\n",
    "    api_key=os.getenv(\"OPENROUTER_API_KEY\"),\n",
    "    temperature=0.2,\n",
    "    max_tokens=800\n",
    ")\n",
    "\n",
    "chain_with_sources = (\n",
    "    {\"context\": retriever | RunnableLambda(parse_docs), \"question\": RunnablePassthrough()}\n",
    "    | RunnablePassthrough().assign(\n",
    "        response=(RunnableLambda(build_prompt) | rag_model | StrOutputParser())\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Running the RAG Pipeline\n",
    "Now we can ask questions to our RAG system. The system will retrieve relevant context (both text and images) and generate an answer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"What is the attention mechanism? can you show me an image of it?\"\n",
    "response = chain_with_sources.invoke(question)\n",
    "\n",
    "print(\"Response:\", response['response'])\n",
    "\n",
    "print(\"\\n\\nContext:\")\n",
    "for text in response['context']['texts']:\n",
    "    print(text.page_content)\n",
    "    if hasattr(text, 'metadata') and 'page_number' in text.metadata:\n",
    "        print(\"Page number: \", text.metadata.page_number)\n",
    "    print(\"\\n\" + \"-\" * 50 + \"\\n\")\n",
    "\n",
    "for image in response['context']['images']:\n",
    "    display_base64_image(image)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
