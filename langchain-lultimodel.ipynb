{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2917df2",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -U \"unstructured[all-docs]\" pillow lxml pillow\n",
    "%pip install -U chromadb tiktoken\n",
    "%pip install -U langchain langchain-community langchain-openai langchain-groq\n",
    "%pip install -U python_dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07b031f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -U langchain-openrouter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07e55143",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()  # take environment variables from .env file"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76d54bb4",
   "metadata": {},
   "source": [
    "Extract the data\n",
    "\n",
    "Extract the elements of the PDF that we will be able to use in the retrieval process. These elements can be: Text, Images, Tables, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb0f3f6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from unstructured.partition.pdf import partition_pdf\n",
    "\n",
    "output_path = \"./content/\"\n",
    "file_path = output_path + 'attention.pdf'\n",
    "\n",
    "\n",
    "chunks = partition_pdf(\n",
    "    filename=file_path,\n",
    "    infer_table_structure=True,            # extract tables\n",
    "    strategy=\"hi_res\",                     # mandatory to infer tables\n",
    "\n",
    "    extract_image_block_types=[\"Image\", \"Table\"],   # Add 'Table' to list to extract image of tables\n",
    "    # image_output_dir_path=output_path,   # if None, images and tables will saved in base64\n",
    "\n",
    "    extract_image_block_to_payload=True,   # if true, will extract base64 for API usage\n",
    "\n",
    "    chunking_strategy=\"by_title\",          # or 'basic'\n",
    "    max_characters=10000,                  # defaults to 500\n",
    "    combine_text_under_n_chars=2000,       # defaults to 0\n",
    "    new_after_n_chars=6000,\n",
    "\n",
    "    \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a8a803a",
   "metadata": {},
   "outputs": [],
   "source": [
    "set([str(type(el)) for el in chunks])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d5bd4c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88504792",
   "metadata": {},
   "outputs": [],
   "source": [
    "chunks[70].metadata.orig_elements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "315af9e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "elements = chunks[3].metadata.orig_elements\n",
    "chunk_images = [el for el in elements if \"Image\" in str(type(el))]\n",
    "chunk_images[0].to_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4955593c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n"
     ]
    }
   ],
   "source": [
    "# counting tables in all chunks\n",
    "chunk_tables = []\n",
    "for i in range(len(chunks)):\n",
    "    elements = chunks[i].metadata.orig_elements\n",
    "    chunk_tables.extend([el for el in elements if \"Table\" in str(type(el))])\n",
    "print(len(chunk_tables))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef7469fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import base64\n",
    "from IPython.display import Image, display\n",
    "\n",
    "def display_base64_image(base64_code):\n",
    "    # Decode the base64 string to binary\n",
    "    image_data = base64.b64decode(base64_code)\n",
    "    # Display the image\n",
    "    display(Image(data=image_data))\n",
    "\n",
    "display_base64_image(images[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a73f90d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate extracted elements into tables, text, and images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "da5273fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# separate tables from texts\n",
    "tables = []\n",
    "texts = []\n",
    "\n",
    "for chunk in chunks:\n",
    "    elements = chunk.metadata.orig_elements\n",
    "    for element in elements:\n",
    "        if \"Table\" in str(type(element)):\n",
    "            tables.append(element)\n",
    "\n",
    "    if \"CompositeElement\" in str(type((chunk))):\n",
    "        texts.append(chunk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23e3d9a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a740217a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the images from the CompositeElement objects\n",
    "def get_images_base64(chunks):\n",
    "    images_b64 = []\n",
    "    for chunk in chunks:\n",
    "        if \"CompositeElement\" in str(type(chunk)):\n",
    "            chunk_els = chunk.metadata.orig_elements\n",
    "            for el in chunk_els:\n",
    "                if \"Image\" in str(type(el)):\n",
    "                    images_b64.append(el.metadata.image_base64)\n",
    "    return images_b64\n",
    "\n",
    "images = get_images_base64(chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a443049",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -Uq langchain-groq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5e6b0c10",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_groq import ChatGroq\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5ee6c33b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Prompt\n",
    "prompt_text = \"\"\"\n",
    "You are an assistant tasked with summarizing tables and text.\n",
    "Give a concise summary of the table or text.\n",
    "\n",
    "Respond only with the summary, no additionnal comment.\n",
    "Do not start your message by saying \"Here is a summary\" or anything like that.\n",
    "Just give the summary as it is.\n",
    "\n",
    "Table or text chunk: {element}\n",
    "\n",
    "\"\"\"\n",
    "prompt = ChatPromptTemplate.from_template(prompt_text)\n",
    "\n",
    "# Summary chain\n",
    "\n",
    "model = ChatGroq(temperature=0.5, model=\"llama-3.1-8b-instant\")\n",
    "\n",
    "\n",
    "summarize_chain = {\"element\": lambda x: x} | prompt | model | StrOutputParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "9a9b7dc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from groq import RateLimitError\n",
    "\n",
    "\n",
    "try:\n",
    "    # Summarize text\n",
    "    text_summaries = summarize_chain.batch(texts, {\"max_concurrency\": 1})\n",
    "\n",
    "    # Summarize tables\n",
    "    tables_html = [table.metadata.text_as_html for table in tables]\n",
    "    table_summaries = summarize_chain.batch(tables_html, {\"max_concurrency\": 1})\n",
    "except RateLimitError as e:\n",
    "    \n",
    "    print(f\"Rate limit exceeded: {e}\")\n",
    "    time.sleep(300)\n",
    "    # Retry summarization\n",
    "    # Summarize text\n",
    "    text_summaries = summarize_chain.batch(texts, {\"max_concurrency\": 1})\n",
    "\n",
    "    # Summarize tables\n",
    "    tables_html = [table.metadata.text_as_html for table in tables]\n",
    "    table_summaries = summarize_chain.batch(tables_html, {\"max_concurrency\": 1})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59a180a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(text_summaries))\n",
    "print(len(table_summaries))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29113705",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -Uq langchain_openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72d9f40b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, re, time\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from google.api_core.exceptions import ResourceExhausted\n",
    "\n",
    "# Make sure you have your API key set:\n",
    "# os.environ[\"GOOGLE_API_KEY\"] = \"your_google_ai_studio_key\"\n",
    "\n",
    "api_key = os.getenv(\"GOOGLE_STUDIO_API_KEY\")\n",
    "\n",
    "# Define prompt\n",
    "prompt_template = \"\"\"Describe the image in detail. For context,\n",
    "                  the image is part of the research paper by google Attention Is All You Need\n",
    "Guide book. Be specific about graphs, such as bar plots.\"\"\"\n",
    "\n",
    "# Template for multimodal message (same as before)\n",
    "messages = [\n",
    "    (\n",
    "        \"user\",\n",
    "        [\n",
    "            {\"type\": \"text\", \"text\": prompt_template},\n",
    "            {\n",
    "                \"type\": \"image_url\",\n",
    "                \"image_url\": {\"url\": \"data:image/jpeg;base64,{image}\"},\n",
    "            },\n",
    "        ],\n",
    "    )\n",
    "]\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages(messages)\n",
    "\n",
    "# Use Gemini 2.0 Flash-Lite (best rate & token limits)\n",
    "llm = ChatGoogleGenerativeAI(\n",
    "    model=\"gemini-2.0-flash-lite\",\n",
    "    temperature=0.5,\n",
    "    google_api_key=api_key,\n",
    ")\n",
    "\n",
    "chain = prompt | llm | StrOutputParser()\n",
    "\n",
    "# Assuming 'images' is a list of base64-encoded images\n",
    "while True:\n",
    "    try:\n",
    "        image_summaries = chain.batch(images)\n",
    "        break  # success, exit loop\n",
    "    except ResourceExhausted as e:\n",
    "        msg = str(e)\n",
    "        # Detect rate limit and extract wait time if possible\n",
    "        match = re.search(r'Please try again in ([\\d\\.]+)([sm])', msg)\n",
    "        wait_seconds = 60  # default wait time\n",
    "        if match:\n",
    "            value, unit = match.groups()\n",
    "            wait_seconds = float(value) * 60 if unit == 'm' else float(value)\n",
    "        print(f\"â³ Rate limit exceeded. Waiting {wait_seconds:.2f} seconds...\")\n",
    "        time.sleep(wait_seconds)\n",
    "    except Exception as e:\n",
    "        print(f\"âš ï¸ Unexpected error: {e}\")\n",
    "        time.sleep(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4d48fb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(image_summaries[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1662f170",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install langchain-huggingface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4b68589",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install sentence-transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f729998",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -Uq InstructorEmbedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "a3fbe948",
   "metadata": {},
   "outputs": [],
   "source": [
    "import uuid\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.storage import InMemoryStore\n",
    "from langchain_core.documents import Document\n",
    "from langchain.embeddings import HuggingFaceBgeEmbeddings\n",
    "from langchain.retrievers.multi_vector import MultiVectorRetriever\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "\n",
    "embeddings = HuggingFaceBgeEmbeddings(\n",
    "    model_name=\"BAAI/bge-large-en-v1.5\",\n",
    "    encode_kwargs={\"normalize_embeddings\": True}\n",
    ")\n",
    "\n",
    "# The vectorstore to index child chunks\n",
    "vectorstore = Chroma(\n",
    "    collection_name=\"multi_modal_rag\",\n",
    "    embedding_function=embeddings\n",
    ")\n",
    "\n",
    "# The storage layer for parent documents\n",
    "store = InMemoryStore()\n",
    "id_key = \"doc_id\"\n",
    "\n",
    "# The retriever (empty to start)\n",
    "retriever = MultiVectorRetriever(\n",
    "    vectorstore=vectorstore,\n",
    "    docstore=store,\n",
    "    id_key=id_key,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "6ea96488",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add texts\n",
    "doc_ids = [str(uuid.uuid4()) for _ in texts]\n",
    "summary_texts = [\n",
    "    Document(page_content=summary, metadata={id_key: doc_ids[i]}) for i, summary in enumerate(text_summaries)\n",
    "]\n",
    "retriever.vectorstore.add_documents(summary_texts)\n",
    "retriever.docstore.mset(list(zip(doc_ids, texts)))\n",
    "\n",
    "# Add tables\n",
    "table_ids = [str(uuid.uuid4()) for _ in tables]\n",
    "summary_tables = [\n",
    "    Document(page_content=summary, metadata={id_key: table_ids[i]}) for i, summary in enumerate(table_summaries)\n",
    "]\n",
    "retriever.vectorstore.add_documents(summary_tables)\n",
    "retriever.docstore.mset(list(zip(table_ids, tables)))\n",
    "\n",
    "# Add image summaries\n",
    "img_ids = [str(uuid.uuid4()) for _ in images]\n",
    "summary_img = [\n",
    "    Document(page_content=summary, metadata={id_key: img_ids[i]}) for i, summary in enumerate(image_summaries)\n",
    "]\n",
    "retriever.vectorstore.add_documents(summary_img)\n",
    "retriever.docstore.mset(list(zip(img_ids, images)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c07da6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Total documents in vectorstore:\", vectorstore._collection.count())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c776a17",
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = vectorstore.get(include=[\"metadatas\", \"documents\"], limit=5)\n",
    "for i, (doc, meta) in enumerate(zip(docs[\"documents\"], docs[\"metadatas\"])):\n",
    "    print(f\"\\n--- Document {i+1} ---\")\n",
    "    print(f\"Metadata: {meta}\")\n",
    "    print(f\"Content: {doc[:300]}...\")  # Print first 300 chars only\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cb8a4e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_embedding = vectorstore._collection.get(limit=1, include=[\"embeddings\"])[\"embeddings\"][0]\n",
    "print(\"Embedding length:\", len(sample_embedding))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "952afa64",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = vectorstore.similarity_search(\"self attention\", k=3)\n",
    "for doc in results:\n",
    "    print(\"\\nðŸ”¹ Retrieved:\", doc.page_content[:300], \"...\")\n",
    "    print(\"Metadata:\", doc.metadata)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05b7d547",
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = vectorstore.get(include=[\"metadatas\"])\n",
    "types = [d.get(\"type\", \"unknown\") for d in docs[\"metadatas\"]]\n",
    "from collections import Counter\n",
    "print(Counter(types))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54d90788",
   "metadata": {},
   "source": [
    "RAG Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "7b1a0b39",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.runnables import RunnablePassthrough, RunnableLambda\n",
    "from langchain_core.messages import HumanMessage\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain.schema.output_parser import StrOutputParser\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from base64 import b64decode\n",
    "import os\n",
    "\n",
    "\n",
    "# --- Helper to separate base64 images from text docs ---\n",
    "def parse_docs(docs):\n",
    "    b64 = []\n",
    "    text = []\n",
    "    for doc in docs:\n",
    "        try:\n",
    "            b64decode(doc)\n",
    "            b64.append(doc)\n",
    "        except Exception:\n",
    "            text.append(doc)\n",
    "    return {\"images\": b64, \"texts\": text}\n",
    "\n",
    "\n",
    "# --- Build the final multimodal prompt ---\n",
    "def build_prompt(kwargs):\n",
    "    docs_by_type = kwargs[\"context\"]\n",
    "    user_question = kwargs[\"question\"]\n",
    "\n",
    "    context_text = \"\"\n",
    "    if len(docs_by_type[\"texts\"]) > 0:\n",
    "        for text_element in docs_by_type[\"texts\"]:\n",
    "            if hasattr(text_element, \"page_content\"):\n",
    "                context_text += text_element.page_content + \"\\n\"\n",
    "            else:\n",
    "                context_text += str(text_element) + \"\\n\"\n",
    "\n",
    "    prompt_template = f\"\"\"\n",
    "    You are an expert assistant. Answer the user's question **only** based on the given context.\n",
    "\n",
    "    Context:\n",
    "    {context_text}\n",
    "\n",
    "    Question:\n",
    "    {user_question}\n",
    "\n",
    "    Provide a clear and concise answer.\n",
    "    \"\"\"\n",
    "\n",
    "    prompt_content = [{\"type\": \"text\", \"text\": prompt_template}]\n",
    "\n",
    "    # attach images if any\n",
    "    if len(docs_by_type[\"images\"]) > 0:\n",
    "        for image in docs_by_type[\"images\"]:\n",
    "            prompt_content.append({\n",
    "                \"type\": \"image_url\",\n",
    "                \"image_url\": {\"url\": f\"data:image/jpeg;base64,{image}\"}\n",
    "            })\n",
    "\n",
    "    return ChatPromptTemplate.from_messages([\n",
    "        HumanMessage(content=prompt_content),\n",
    "    ])\n",
    "\n",
    "\n",
    "# --- Build the chain ---\n",
    "chain = (\n",
    "    {\n",
    "        \"context\": retriever | RunnableLambda(parse_docs),\n",
    "        \"question\": RunnablePassthrough(),\n",
    "    }\n",
    "    | RunnableLambda(build_prompt)\n",
    "    | ChatOpenAI(\n",
    "        model=\"qwen/qwen2.5-vl-32b-instruct:free\",  # Qwen model on OpenRouter\n",
    "        base_url=\"https://openrouter.ai/api/v1\",\n",
    "        api_key=os.getenv(\"OPENROUTER_API_KEY\"),\n",
    "        temperature=0.2,\n",
    "        max_tokens=800\n",
    "    )\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "# --- Optional: RAG with source output ---\n",
    "chain_with_sources = (\n",
    "    {\n",
    "        \"context\": retriever | RunnableLambda(parse_docs),\n",
    "        \"question\": RunnablePassthrough(),\n",
    "    }\n",
    "    | RunnablePassthrough().assign(\n",
    "        response=(\n",
    "            RunnableLambda(build_prompt)\n",
    "            | ChatOpenAI(\n",
    "                model=\"qwen/qwen2.5-vl-32b-instruct:free\",\n",
    "                base_url=\"https://openrouter.ai/api/v1\",\n",
    "                api_key=os.getenv(\"OPENROUTER_API_KEY\"),\n",
    "                temperature=0.2,\n",
    "                max_tokens=800\n",
    "            )\n",
    "            | StrOutputParser()\n",
    "        )\n",
    "    )\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b1630a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = chain_with_sources.invoke(\"What is the attention mechanism? can you show me an image fo it?\")\n",
    "print(\"Response:\", response['response'])\n",
    "\n",
    "print(\"\\n\\nContext:\")\n",
    "for text in response['context']['texts']:\n",
    "    print(text.text)\n",
    "    print(\"Page number: \", text.metadata.page_number)\n",
    "    print(\"\\n\" + \"-\"*50 + \"\\n\")\n",
    "for image in response['context']['images']:\n",
    "    display_base64_image(image)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
